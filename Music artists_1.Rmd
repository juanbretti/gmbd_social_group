---
title: "SOCIAL NETWORKS ANALYSIS (MBD-EN-BL2020J-1_32R220_387064)"
author: "Alberto I., Daniel R., Iman H., Juan Pedro B., Pablo B., Vania M."
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float:
      collapsed: true
---

```{css, echo=FALSE}
.html-widget {
    margin: auto;
}
body .main-container {
  max-width: 1280px !important;
  width: 1280px !important;
}
body {
  max-width: 1280px !important;
}
toc-content {
    max-width: 1280px !important;
}
```

```{r setup, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(message = FALSE, echo=TRUE, warning=FALSE)
```

# Description

The final assignment consist in the practical analysis of a dataset you
pick. Please, choose as sooner as possible and send it to me. I will
advise if it is difficult or not.

You can analyse any dataset, at least you should try to discover who are
the most important nodes of the graph, analyse some properties of the
graph (diameter, avg pathe length, histograms and plots of some metrics,
etc.), try to find communities. Other aspects you could try involve
specific analysis and descriptives of your datasets, link prediction
(similar to recommendation engines), sentiment analysis of each group,
information diffussion, usage of metrics or techniques not seen in
class, etc.

In the report will be valued the originality, the use of the techniques
shown during the module and the complexity of them, the quality of
insights you get, the complexity of the graph, the correctness, etc.

The presentation will consist of an exposition of 10 minutes and 3-4
minutes of questions (mainly from other students). And it will be valued
the knowledge of the material, the organization, the clarity of the
explanations, the impact of graphs and slides, the timing, etc.

Deliverable consist of the PPT of the exposition, the analysis in
Rmd/python and a report with the explanations (HTML or pdf).

# Environment
## Load libraries

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(igraph)
library(caret)
library(PerformanceAnalytics)
library(networkD3)
```

```{r}
theme_set(theme_minimal(base_size = 20))
```

## Load data

```{r, warning=FALSE, message=FALSE}
data_music <- read_csv('data/full_music_data.csv')
data_influence <- read_csv('data/influence_data.csv')
data_artist <- read_csv('data/data_by_artist.csv')
```

Reformat `songs` data.frame

```{r}
data_influence <- data_influence %>% 
  mutate(follower_main_genre = str_remove_all(follower_main_genre, ';'))

artist_genre <- data_influence %>% 
  select(`follower_name`, `follower_main_genre`) %>% 
  unique() %>% 
  rename(`artist_names`=`follower_name`, `main_genre`=`follower_main_genre`)

data_music_genre <- data_music %>%
  transform(artists_id = strsplit(str_trim(str_remove_all(artists_id, '[\\[\\]]'), side='both'), ","),
            artist_names = strsplit(str_trim(str_remove_all(artist_names, '[\\[\\]]'), side='both'), ",")) %>%
  unnest(artist_names) %>% 
  mutate(artist_names = gsub('^.|.$', '', artist_names)) %>% 
  inner_join(artist_genre, by="artist_names")
```

Problem with unknown artist

```{r}
unique_a <- unique(data_artist['artist_id'])
unique_b <- unique(data_influence['influencer_id'])
unique_c <- unique(data_influence['follower_id'])

table(unlist(unique_b) %in% unlist(unique_a))
table(unlist(unique_c) %in% unlist(unique_a))
```

```{r}
unknown_artist <- unique_c[!(unlist(unique_c) %in% unlist(unique_a)),]
```

## Calculate `PCA`

```{r}
# colnames(data_artist)
columns_to_normalize <- c("danceability", "energy", "valence", "tempo", "loudness", "mode", "key", "acousticness", "instrumentalness", "liveness", "speechiness", "duration_ms", "popularity", "count")

data <- data_artist[, columns_to_normalize]
tranformation_1 <- preProcess(data, method=c("scale", "pca"), pcaComp=3)
data_artist$PC1 <- predict(tranformation_1, data)[['PC1']]
data_artist$PC2 <- predict(tranformation_1, data)[['PC2']]
data_artist$PC3 <- predict(tranformation_1, data)[['PC3']]

data_influence_transformed <- data_influence %>% 
  inner_join(data_artist[, c('artist_id', 'PC1', 'PC2', 'PC3')], by = c("influencer_id" = "artist_id")) %>% 
  inner_join(data_artist[, c('artist_id', 'PC1', 'PC2', 'PC3')], by = c("follower_id" = "artist_id"), suffix = c("_influencer", "_follower")) %>% 
  rowwise() %>%
  mutate(`weight` = dist(matrix(c(`PC1_influencer`, `PC1_follower`, `PC2_influencer`, `PC2_follower`, `PC3_influencer`, `PC3_follower`), nrow = 2)))
```


```{r}
vars <- apply(tranformation_1$rotation, 2, var)
vars/sum(vars)

pc1 <- sum(tranformation_1$rotation[,1])
pc2 <- sum(tranformation_1$rotation[,2])
pc3 <- sum(tranformation_1$rotation[,3])

pc1/sum(pc1+pc2+pc3)
```


## Create the graph

```{r}
data_influence_edges <- data_influence_transformed %>% 
  rename(from=influencer_id, to=follower_id) %>% 
  relocate(from, to, weight)

data_artist_vertices <- data_artist %>% 
  rename(name=artist_name) %>% 
  relocate(artist_id)

g <- graph_from_data_frame(d=data_influence_edges, vertices=data_artist_vertices, directed=TRUE) 
```

```{r}
# https://stackoverflow.com/questions/44892923/how-to-increase-length-of-edges-when-plotting-a-graph-in-r
# https://stackoverflow.com/questions/32012080/changing-the-spacing-between-vertices-in-igraph-in-r
# test.layout <- layout_(g, with_dh(weight.edge.lengths = edge_density(g)/1000))
# plot(g, layout = test.layout)
# plot(g)
```

# EDA
## Duration evolution

```{r}
top_ <- 5

sorted_music_genre <- data_music_genre %>% 
  group_by(`main_genre`) %>% 
  tally() %>% 
  arrange(desc(n)) %>% 
  select(`main_genre`) %>% 
  unlist(use.names=FALSE)

data_music_genre %>% 
  mutate(main_genre = factor(main_genre, levels=sorted_music_genre)) %>% 
  filter(main_genre %in% sorted_music_genre[1:top_]) %>% 
  group_by(`year`, `main_genre`) %>% 
  summarise(mean=mean(duration_ms)/1000/60) %>% 
  ggplot(aes(x=`year`, y=`mean`, color=`main_genre`)) +
  geom_line() +
  ylim(c(1.5, 5)) +
  labs(title='Song duration over time per genre', x=NULL, y='Minutes', color='Genre')+
  theme_light()

data_music_genre %>% 
  mutate(main_genre = factor(main_genre, levels=sorted_music_genre)) %>% 
  filter(main_genre %in% sorted_music_genre[1:top_]) %>% 
  ggplot(aes(x=`year`, y=`duration_ms`/1000/60)) +
  geom_smooth(aes(color=`main_genre`), method="lm", formula=y~poly(x, 3, raw=TRUE)) +
  ylim(c(0, 5)) +
  labs(title='Song duration over time', x=NULL, y='Minutes', color='Genre') +
  theme_light()
```

## Tempo evolution

```{r}
data_music_genre %>% 
  mutate(main_genre = factor(main_genre, levels=sorted_music_genre)) %>% 
  filter(main_genre %in% sorted_music_genre[1:top_]) %>% 
  ggplot(aes(x=`year`, y=`tempo`)) +
  geom_smooth(aes(color=`main_genre`), method="lm", formula=y~poly(x, 3, raw=TRUE)) +
  # ylim(c(0, 5)) +
  labs(title='Song tempo over time', x=NULL, y='Tempo', color='Genre')
```

## Top influencer artists

```{r}
top_influencers_ <- 5

data_influence_sorted <- data_influence %>% 
  group_by(`influencer_name`) %>% 
  tally() %>% 
  arrange(desc(n)) %>% 
  select(`influencer_name`) %>% 
  unlist(use.names=FALSE)

data_influence %>% 
  mutate(influencer_name = factor(influencer_name, levels=data_influence_sorted)) %>% 
  dplyr::filter(influencer_name %in% data_influence_sorted[1:top_]) %>% 
  group_by(`influencer_name`) %>% 
  tally() %>% 
  ggplot(aes(x=`influencer_name`, y=`n`)) +
  geom_col() +
  coord_flip() +
  scale_x_discrete(limits = rev) +
  labs(title='Top influencer artists', x=NULL, y='Number of influenced')

data_influence %>% 
  mutate(follower_main_genre = factor(follower_main_genre, levels=sorted_music_genre)) %>% 
  filter(follower_main_genre %in% sorted_music_genre[1:top_]) %>% 
  group_by(`follower_main_genre`, `influencer_name`) %>%
  tally() %>% 
  arrange(desc(n)) %>% 
  slice(1:top_influencers_) %>% 
  ggplot(aes(x=reorder(`influencer_name`, -n, sum), y=`n`)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  scale_x_discrete(limits = rev) +
  facet_wrap(`follower_main_genre` ~ ., scales="free_y") +
  labs(title='Top influencer artists by genre', x=NULL, y='Number of influenced')
```

## Source of inspiration by genre

```{r}
data_influence %>% 
  mutate(influencer_main_genre = factor(influencer_main_genre, levels=sorted_music_genre),
         follower_main_genre = factor(follower_main_genre, levels=sorted_music_genre)) %>% 
  filter(influencer_main_genre %in% sorted_music_genre[1:top_], follower_main_genre %in% sorted_music_genre[1:top_]) %>% 
  group_by(`influencer_main_genre`, `follower_main_genre`) %>%
  tally() %>% 
  ggplot(aes(x=reorder(`influencer_main_genre`, -n, sum), y=reorder(`follower_main_genre`, -n, sum), fill=log(`n`))) +
  geom_tile() +
  scale_x_discrete(guide = guide_axis(angle = 90)) +
  labs(title='Source of inspiration by genre', x='Influencer', y='Follower', fill=NULL) +
  scale_fill_gradient(low="#F2D544", high="#025959")
```


# Description

## Order (number of vertices)
https://igraph.org/r/doc/gorder.html

```{r}
gorder(g)
```

## Size of the graph (number of edges)
https://igraph.org/r/doc/gsize.html

```{r}
gsize(g)
```

## Degree distribution

```{r}
deg <- degree(g, mode="all", loop=FALSE)
hist(deg, main='Total degree distribution')
```

## Degree description

```{r}
summary(deg)
```

## Network Diameter and Average Path Length

```{r}
diameter(g)
```

## Average path length of the graph

```{r}
mean_distance(g)
```

## Centrality Measurements

In the case of `closeness`, there is a *warning* while having a disconnected graph: \
`closeness centrality is not well-defined for disconnected graphs`\
This can be saved, by finding the biggest subgraph, and only calculate the `closeness` of that.

```{r}
# https://stackoverflow.com/a/55891333/3780957
g_components <- components(g, mode="strong")

biggest_component <- which.max(g_components$csize)
g_main <- induced_subgraph(g, which(g_components$membership == biggest_component))
cl <- closeness(g_main, normalized=TRUE)
head(sort(cl, decreasing=TRUE), 5)
```

To follow the class exercise, the following line ignores for the `closeness` the `warning`, and executes the code for the full graph.

```{r}
dg <- degree(g, mode = "total", loop=FALSE, normalized=TRUE)
bt <- betweenness(g, normalized=TRUE)
cl <- closeness(g, normalized=TRUE)
pr <- page_rank(g)$vector

cat('Degree\n')
head(sort(dg, decreasing=TRUE), 5)

cat('\n\nBetweenness\n')
head(sort(bt, decreasing=TRUE), 5)

cat('\n\nCloseness\n')
head(sort(cl, decreasing=TRUE), 5)

cat('\n\nPage Rank\n')
head(sort(pr, decreasing=TRUE), 5)
```

### Correlation

The following plot, checks the correlation between the tree calculated centrality measures: \
* Degree \
* Betweenness \
* Closeness \
* Page Rank \
There is a slight posirive correlation of `0.34` between `degree` and `betweenness`.\
The values of the metrics are not normalized neither scaled.

```{r}
results <- data.frame(Degree=dg, Betweenness=bt, Closeness=cl, `Page Rank`=pr) 
chart.Correlation(results, histogram=TRUE, pch=19)
```

### Top `Betweenness`

List of artists with the largest `betweenness` centrality. 

```{r}
top_betweenness <- head(sort(bt, decreasing=TRUE), 10)

data_artist_vertices$bt <- bt

data_artist_vertices %>% 
  filter(`name` %in% names(top_betweenness)) %>% 
  select(`name`, `popularity`, `count`) %>% 
  arrange(factor(`name`, levels = names(top_betweenness))) %>% 
  rename(`Artist name` = `name`, `Popularity` = `popularity`, `Number of songs` = `count`)
```

# Comunity detection
Using https://igraph.org/r/doc/cluster_louvain.html \
More details at https://en.wikipedia.org/wiki/Louvain_method
\
The different methods for finding communities, they all return a communities object: `cluster_edge_betweenness`, `cluster_fast_greedy`, `cluster_label_prop`, `cluster_leading_eigen`, `cluster_louvain`, `cluster_optimal`, `cluster_spinglass`, `cluster_walktrap.`

```{r}
# Communities can only be calculated for undirected graphs
g_undirected <- as.undirected(g, mode='collapse', edge.attr.comb="max")
```

```{r}
cluster_test <- list(
  # cluster_edge_betweenness=cluster_edge_betweenness(g_undirected), 
  cluster_fast_greedy=cluster_fast_greedy(g_undirected),
  # cluster_label_prop=cluster_label_prop(g_undirected),
  cluster_leading_eigen=cluster_leading_eigen(g_undirected),
  cluster_louvain=cluster_louvain(g_undirected),
  # cluster_optimal=cluster_optimal(g_undirected) # Super slow
  # cluster_spinglass=cluster_spinglass(g), # Cannot work with unconnected graph, Invalid value
  cluster_walktrap=cluster_walktrap(g_undirected),
  walktrap.community=walktrap.community(g_undirected)
)

lapply(cluster_test, modularity)
```

Best performance community calculation method

```{r}
g_communities <- cluster_test$cluster_louvain
```

`modularity` measures how good the division is, or how separated are the different vertex types from each other.

```{r}
modularity(g_communities)
```

Calculates the `modularity` of a graph with respect to the given `membership` vector.

```{r}
modularity(g_communities, membership(g_communities))
```

Number of communities

```{r}
length(g_communities)
```

Number of edges inside a community

```{r}
hist(sizes(g_communities), main='Number of actors inside a community', xlab='Members')
```

Length per `community`

```{r}
x <- sapply(g_communities[], length)
summary(x)
which.max(x)
```

## Validation if the vertex labels per community makes sense

```{r}
g_communities[]$`66`
```

# Case study
## Filter

Remove vertices
<https://stackoverflow.com/questions/29784095/conditional-removing-of-vertices-based-on-attributes-in-r>
<https://stackoverflow.com/questions/53696798/filter-igraph-object-by-vertex-attribute-value>

From
<https://stackoverflow.com/questions/3305669/how-do-i-find-the-edges-of-a-vertex-using-igraph-and-r>

```{r}
g1 <- induced_subgraph(g, V(g)$danceability >= 0.85)
g1 <- induced_subgraph(g, V(g)$tempo >= 180)
# plot(g1)
```

Remove edges
<https://stackoverflow.com/questions/25413633/igraph-in-r-how-to-select-edges-based-on-incident-vertex-attributes>
<https://igraph.org/r/doc/subgraph.html>

```{r}
# Nat King Cole	 
filter_idx <- match("Enrique Iglesias", V(g)$name)
filter_edges <- as.numeric(E(g)[from(filter_idx)])
g1 <- subgraph.edges(g, filter_edges, delete.vertices = TRUE)
                       
plot(g1)
```

## Topology-Ego Filter
https://stackoverflow.com/a/39992337/3780957

```{r}
# g_ego <- make_ego_graph(g, order = 1, nodes = 'Gloria Estefan', mode = "all", mindist = 0)
g_ego <- make_ego_graph(g, order = 1, nodes = 'Enrique Iglesias', mode = "all", mindist = 0)
# g_ego <- make_ego_graph(g, order = 1, nodes = 'Ricky Martin', mode = "all", mindist = 0)
# g_ego <- make_ego_graph(g, order = 1, nodes = 'Madonna', mode = "all", mindist = 0)
g_ego <- g_ego[[1]]
```

### Interactive plot
https://juanitorduz.github.io/text-mining-networks-and-visualization-plebiscito-tweets/

```{r}
plot_network_3D <- function(network, arrow=TRUE) {
  # Store the degree.
  V(network)$degree <- strength(graph = network)
  # Create networkD3 object.
  network.D3 <- igraph_to_networkD3(g = network)
  # Define node size.
  network.D3$nodes <- network.D3$node %>% mutate(Degree = V(network)$degree, Group = 1)
  # Define edges width. 
  network.D3$links$Width <- 1
  
  forceNetwork(
    Links = network.D3$links, 
    Nodes = network.D3$nodes, 
    Source = 'source', 
    Target = 'target',
    NodeID = 'name',
    Group = 'Group',
    opacity = 0.9,
    Value = 'Width',
    Nodesize = 'Degree',
    # We input a JavaScript function.
    linkWidth = JS("function(d) { return Math.sqrt(d.value); }"), 
    fontSize = 12,
    arrows = arrow,
    zoom = TRUE, 
    opacityNoHover = 1
  )
}

plot(g_ego, edge.arrow.size=.2)
plot_network_3D(g_ego)
```

## Shortest path

```{r}
short <- all_shortest_paths(g, from='Enrique Iglesias', to = 'Wolfgang Amadeus Mozart', mode = "all")
g1 <- induced_subgraph(g, short$res[[1]])
plot(g1)
plot_network_3D(g1)
```

## Color boundary for communities
https://stackoverflow.com/questions/37374355/how-can-i-plot-igraph-community-with-defined-colors

```{r}
g_undirected <- as.undirected(g_ego, mode='collapse', edge.attr.comb="max")
wc <- cluster_louvain(g_undirected)

layout <-layout.fruchterman.reingold(g_ego)
par(mar=c(0,0,0,0)+.1)

plot(wc, g_ego, layout=layout, vertex.size=5,  edge.arrow.size=.2)
```

# Predict edges

Based on a hierarchical random graph model.\
`predict_edges` uses a hierarchical random graph model to predict missing edges from a network. This is done by sampling hierarchical models around the optimum model, proportionally to their likelihood.

https://igraph.org/r/doc/predict_edges.html

Based on a smaller graph, centered in 'Enrique Iglesias'.

```{r}
hrg <- fit_hrg(g_ego)
hrg
```

```{r}
g_ego_predicted <- predict_edges(g_ego, hrg=hrg)
```

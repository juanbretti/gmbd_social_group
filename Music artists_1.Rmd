---
title: "SOCIAL NETWORKS ANALYSIS (MBD-EN-BL2020J-1_32R220_387064)"
author: "Alberto I., Daniel R., Iman H., Juan Pedro B., Pablo B., Vania M."
output: html_document
---

# Description

The final assignment consist in the practical analysis of a dataset you
pick. Please, choose as sooner as possible and send it to me. I will
advise if it is difficult or not.

You can analyse any dataset, at least you should try to discover who are
the most important nodes of the graph, analyse some properties of the
graph (diameter, avg pathe length, histograms and plots of some metrics,
etc.), try to find communities. Other aspects you could try involve
specific analysis and descriptives of your datasets, link prediction
(similar to recommendation engines), sentiment analysis of each group,
information diffussion, usage of metrics or techniques not seen in
class, etc.

In the report will be valued the originality, the use of the techniques
shown during the module and the complexity of them, the quality of
insights you get, the complexity of the graph, the correctness, etc.

The presentation will consist of an exposition of 10 minutes and 3-4
minutes of questions (mainly from other students). And it will be valued
the knowledge of the material, the organization, the clarity of the
explanations, the impact of graphs and slides, the timing, etc.

Deliverable consist of the PPT of the exposition, the analysis in
Rmd/python and a report with the explanations (HTML or pdf).

# Environment
## Load libraries

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(igraph)
library(caret)
library(PerformanceAnalytics)
```

## Load data

```{r, warning=FALSE, message=FALSE}
data_music <- read_csv('data/full_music_data.csv')
data_influence <- read_csv('data/influence_data.csv')
data_artist <- read_csv('data/data_by_artist.csv')
```

Reformat `soungs` data.frame

```{r}
data_music %>% 
  transform(artists_id = strsplit(str_trim(str_remove_all(artists_id, '[\\[\\]]'), side='both'), ",")) %>% 
  unnest(artists_id)
  # dplyr::filter(grepl(',', artist_names))
```

Problem with unknown artist

```{r}
unique_a <- unique(data_artist['artist_id'])
unique_b <- unique(data_influence['influencer_id'])
unique_c <- unique(data_influence['follower_id'])

table(unlist(unique_b) %in% unlist(unique_a))
table(unlist(unique_c) %in% unlist(unique_a))
```

```{r}
unknown_artist <- unique_c[!(unlist(unique_c) %in% unlist(unique_a)),]
```

## Calculate `PCA`

```{r}
# colnames(data_artist)
columns_to_normalize <- c("danceability", "energy", "valence", "tempo", "loudness", "mode", "key", "acousticness", "instrumentalness", "liveness", "speechiness", "duration_ms", "popularity", "count")

data <- data_artist[, columns_to_normalize]
tranformation_1 <- preProcess(data, method=c("scale", "pca"), pcaComp=3)
data_artist$PC1 <- predict(tranformation_1, data)[['PC1']]
data_artist$PC2 <- predict(tranformation_1, data)[['PC2']]
data_artist$PC3 <- predict(tranformation_1, data)[['PC3']]

data_influence_transformed <- data_influence %>% 
  inner_join(data_artist[, c('artist_id', 'PC1', 'PC2', 'PC3')], by = c("influencer_id" = "artist_id")) %>% 
  inner_join(data_artist[, c('artist_id', 'PC1', 'PC2', 'PC3')], by = c("follower_id" = "artist_id"), suffix = c("_influencer", "_follower")) %>% 
  rowwise() %>%
  mutate(`weight` = dist(matrix(c(`PC1_influencer`, `PC1_follower`, `PC2_influencer`, `PC2_follower`, `PC3_influencer`, `PC3_follower`), nrow = 2)))
```

## Create the graph

```{r}
data_influence_edges <- data_influence_transformed %>% 
  rename(from=influencer_id, to=follower_id) %>% 
  relocate(from, to, weight) %>% 
  filter(!(to %in% unknown_artist))

data_artist_vertices <- data_artist %>% 
  rename(name=artist_name) %>% 
  relocate(artist_id)

g <- graph_from_data_frame(d=data_influence_edges, vertices=data_artist_vertices, directed=TRUE) 
```

```{r}
# https://stackoverflow.com/questions/44892923/how-to-increase-length-of-edges-when-plotting-a-graph-in-r
# https://stackoverflow.com/questions/32012080/changing-the-spacing-between-vertices-in-igraph-in-r
# test.layout <- layout_(g, with_dh(weight.edge.lengths = edge_density(g)/1000))
# plot(g, layout = test.layout)
# plot(g)
```

# EDA
## Filter

Remove vertices
<https://stackoverflow.com/questions/29784095/conditional-removing-of-vertices-based-on-attributes-in-r>
<https://stackoverflow.com/questions/53696798/filter-igraph-object-by-vertex-attribute-value>

From
<https://stackoverflow.com/questions/3305669/how-do-i-find-the-edges-of-a-vertex-using-igraph-and-r>

```{r}
# g1 <- delete_vertices(g, V(g)$danceability >= 0.9)

g1 <- induced_subgraph(g, V(g)$danceability >= 0.85)
g1 <- induced_subgraph(g, V(g)$tempo >= 180)
# plot(g1)
```

Remove edges
<https://stackoverflow.com/questions/25413633/igraph-in-r-how-to-select-edges-based-on-incident-vertex-attributes>
<https://igraph.org/r/doc/subgraph.html>

```{r}
# Nat King Cole	 
filter_idx <- match("Enrique Iglesias", V(g)$name)
filter_edges <- as.numeric(E(g)[from(filter_idx)])
g1 <- subgraph.edges(g, filter_edges, delete.vertices = TRUE)
                       
plot(g1)
```

### Topology-Ego Filter
https://stackoverflow.com/a/39992337/3780957

```{r}
# g_ego <- make_ego_graph(g, order = 1, nodes = 'Gloria Estefan', mode = "all", mindist = 0)
g_ego <- make_ego_graph(g, order = 1, nodes = 'Enrique Iglesias', mode = "all", mindist = 0)
# g_ego <- make_ego_graph(g, order = 1, nodes = 'Ricky Martin', mode = "all", mindist = 0)
# g_ego <- make_ego_graph(g, order = 1, nodes = 'Madonna', mode = "all", mindist = 0)
g_ego <- g_ego[[1]]
plot(g_ego, edge.arrow.size=.2)
```

### Shortest path

```{r}
short <- all_shortest_paths(g, from='Enrique Iglesias', to = 'Kiss', mode = "all")
short$res[[1]]
```

# Description

## Order (number of vertices)
https://igraph.org/r/doc/gorder.html

```{r}
gorder(g)
```

## Size of the graph (number of edges)
https://igraph.org/r/doc/gsize.html

```{r}
gsize(g)
```

## Degree distribution

```{r}
deg <- degree(g, mode="all", loop=FALSE)
hist(deg, main='Total degree distribution')
```

## Degree description

```{r}
summary(deg)
```

## Network Diameter and Average Path Length

```{r}
diameter(g)
```

## Average path length of the graph

```{r}
mean_distance(g)
```

## Centrality Measurements

In the case of `closeness`, there is a *warning* while having a disconnected graph: \
`closeness centrality is not well-defined for disconnected graphs`\
This can be saved, by finding the biggest subgraph, and only calculate the `closeness` of that.

```{r}
# https://stackoverflow.com/a/55891333/3780957
g_components <- components(g, mode="strong")

biggest_component <- which.max(g_components$csize)
g_main <- induced_subgraph(g, which(g_components$membership == biggest_component))
cl <- closeness(g_main, normalized=TRUE)
head(sort(cl, decreasing=TRUE), 5)
```

To follow the class exercise, the following line ignores for the `closeness` the `warning`, and executes the code for the full graph.

```{r}
dg <- degree(g, mode = "total", loop=FALSE, normalized=TRUE)
bt <- betweenness(g, normalized=TRUE)
cl <- closeness(g, normalized=TRUE)

cat('Degree\n')
head(sort(dg, decreasing=TRUE), 5)

cat('\n\nBetweenness\n')
head(sort(bt, decreasing=TRUE), 5)

cat('\n\nCloseness\n')
head(sort(cl, decreasing=TRUE), 5)
```

### Correlation

The following plot, checks the correlation between the tree calculated centrality measures: \
- Degree \
- Betweenness \
- Closeness \
There is a slight posirive correlation of `0.39` between `degree` and `betweenness`.\
The values of the metrics are not normalized neither scaled.

```{r}
results <- data.frame(Degree=dg, Betweenness=bt, Closeness=cl) 
chart.Correlation(results, histogram=TRUE, pch=19)
```

### Top `Betweenness`

List of artists with the largest `betweenness` centrality. 

```{r}
top_betweenness <- head(sort(bt, decreasing=TRUE), 10)

data_artist_vertices$bt <- bt

data_artist_vertices %>% 
  filter(`name` %in% names(top_betweenness)) %>% 
  select(`name`, `popularity`, `count`) %>% 
  arrange(factor(`name`, levels = names(top_betweenness))) %>% 
  rename(`Artist name` = `name`, `Popularity` = `popularity`, `Number of songs` = `count`)
```

# Comunity detection
Using https://igraph.org/r/doc/cluster_louvain.html \
More details at https://en.wikipedia.org/wiki/Louvain_method
\
The different methods for finding communities, they all return a communities object: `cluster_edge_betweenness`, `cluster_fast_greedy`, `cluster_label_prop`, `cluster_leading_eigen`, `cluster_louvain`, `cluster_optimal`, `cluster_spinglass`, `cluster_walktrap.`

```{r}
# Communities can only be calculated for undirected graphs
g_undirected <- as.undirected(g, mode='collapse', edge.attr.comb="max")

# Calculate the weight, using the distance in years between the follower and influencer
# E(g_undirected)$weight = E(g_undirected)$follower_active_start - E(g_undirected)$influencer_active_start
```

```{r}
cluster_test <- list(
  # cluster_edge_betweenness=cluster_edge_betweenness(g_undirected), 
  cluster_fast_greedy=cluster_fast_greedy(g_undirected),
  # cluster_label_prop=cluster_label_prop(g_undirected),
  cluster_leading_eigen=cluster_leading_eigen(g_undirected),
  cluster_louvain=cluster_louvain(g_undirected),
  # cluster_optimal=cluster_optimal(g_undirected) # Super slow
  # cluster_spinglass=cluster_spinglass(g), # Cannot work with unconnected graph, Invalid value
  cluster_walktrap=cluster_walktrap(g_undirected)
)

lapply(cluster_test, modularity)
```

Best performance community calculation method

```{r}
g_communities <- cluster_test$cluster_louvain
```

`modularity` measures how good the division is, or how separated are the different vertex types from each other.

```{r}
modularity(g_communities)
```

Calculates the `modularity` of a graph with respect to the given `membership` vector.

```{r}
modularity(g_communities, membership(g_communities))
```

Number of communities

```{r}
length(g_communities)
```

Number of edges inside a community

```{r}
hist(sizes(g_communities), main='Number of actors inside a community', xlab='Members')
```

Length per `community`

```{r}
x <- sapply(g_communities[], length)
summary(x)
which.max(x)
```

## Validation if the vertex labels per community makes sense

```{r}
g_communities[]$`3`
```

#Predict edges based on a hierarchical random graph model

https://igraph.org/r/doc/predict_edges.html

Based on a smaller graph, centered in 'Enrique Iglesias'.

```{r}
g_ego <- make_ego_graph(g, order = 1, nodes = 'Enrique Iglesias', mode = "all", mindist = 0)
g_ego <- g_ego[[1]]
plot(g_ego, edge.arrow.size=.2)
```

```{r}
hrg <- fit_hrg(g_ego)
hrg
```

```{r}
g_ego_predicted <- predict_edges(g_ego, hrg=hrg)
```

Ideas

<https://github.com/jdwilson4/Network-Analysis-I/blob/master/Code/Descriptive_Analysis.R>
